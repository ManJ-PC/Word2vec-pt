{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManJ-PC/Word2vec-pt/blob/master/arize_wide_llm_kaggle_example_v2_0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lgvY4Vdx3N_"
      },
      "source": [
        "# LLM on Tabular Data: Kaggle Competition\n",
        "\n",
        "by: https://towardsdatascience.com/boosting-tabular-data-predictions-with-large-language-models-531337f834dc\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This effort is a first-of-its-kind use of a large language model (LLM) on tabular data. We show how a simple prompts created from a single observations of tabular data can be used to make predictions using large language models. This requires little to no data cleansing or feature engineering. Although current results are less than state-of-the-art, the minimal effort still places respectably in the rankings of a Kaggle competition.\n",
        "\n",
        "It is perhaps counterintuitive that a LLM can produce predictions on tabular data.    \n",
        "\n",
        "To demonstrate our methodology, we turn to Kaggle.com. Kaggle hosts competitions involving a specific problem or challenge related to data analysis or machine learning. Competitors can download sponsor provided data, build predictive models using the data, and submit predictions to Kaggle for evaluation. The submissions are blind, meaning competitors do not know the true outcomes. Submissions are evaluated based on predefined evaluation metrics, and the participant with the highest score or accuracy is declared the winner.\n",
        "\n",
        "Kaggle competitions provide a means to evaluate the competitive performance of various modeling techniques. In the demo below, we have used a large language model with minimal data cleaning and feature development. We have then submitted the evaluations data and had the Kaggle competition evaluate our result.  \n",
        "\n",
        "The competition that our model entered is the \"Home Prices Advanced Regression Techniques\" competition. This is one of the more popular competitions attracting 4,229 teams, 4,486 competitors, and 24,123 blind submissions.\n",
        "\n",
        "With little effort Kaggle placed our LLM only prediction in the 86.3 percentile; a respectable showing for a first attempt with little effort. This, of course, does not exceed or beat the state-of-the-art, but did surprisingly well given the little background knowledge, effort, or art used to achieve those results.\n",
        "\n",
        "In the next section, we discuss the data and its preparation.  Section III describes the creation of the prompt and the retrival of the embedding vectors. Section IV, describes simple modeling efforts.  Section V. discuss the model results.  Section VI provides some concluding remarks.\n",
        "\n",
        "References:\n",
        "* [House Prices Advanced Regrssion Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRkzmjYyACKJ"
      },
      "source": [
        "## Methodology\n",
        "\n",
        "The process is fairly intuitive involving the following steps:\n",
        "\n",
        "1. Formulate a prompt for each observation (row of data).  \n",
        "2. Submit the prompt to the LLM and retrieve the embedding vector.\n",
        "3. Use the embedding vectors as train a traditional supervised regression model.  \n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEggCpyi_8l-"
      },
      "source": [
        "### Imports\n",
        "\n",
        "These are the global imports used throughout this demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pXsKTCemTkMt"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import uuid\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "#  import seaborn as sns\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "## sklearn\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score,mean_squared_log_error\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn import set_config\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression, LassoLars, SGDRegressor, Ridge, LogisticRegression, BayesianRidge\n",
        "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ1pRNJMAI1o"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HJfvPhFqgFhg"
      },
      "outputs": [],
      "source": [
        "set_config(display='diagram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TjNJjiZLiPEn"
      },
      "outputs": [],
      "source": [
        "# Model ID: 'kaggle-house-prices-llm%Y%m%d-%H:%M'\n",
        "now = datetime.now()\n",
        "run_str = now.strftime('%Y%m%d-%H:%M')\n",
        "model_id = \"kaggle-house-prices-llm-\" + run_str\n",
        "model_version = \"test\"\n",
        "\n",
        "def gen_model_id( prefix=\"kaggle-house-prices-llm-\", timestamp=datetime.now().strftime('%Y%m%d-%H:%M'), model_type=\"regr\" ):\n",
        "  return( prefix + model_type + '-' + timestamp )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQzw5Cp3JF7O"
      },
      "source": [
        "### Arize\n",
        "\n",
        "Initialize and load Arize client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "hml_WlmqJRr_",
        "outputId": "de33f8cc-49b7-465a-fda9-be2ead37cb73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.5/125.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "AuthError",
          "evalue": "Arize Client could not obtain credentials. You can pass your api_key and space_key directly to the Arize Client, or you can set environment variables which will be read if the keys are not directly passed. To set the environment variables use the following variable names: \n - ARIZE_API_KEY for the api key\n - ARIZE_SPACE_KEY for the space key\nMissing: ['api_key', 'space_key']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-924ead5fbacd>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Arize Client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0marize_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSPACE_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAPI_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/arize/pandas/logger.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, space_key, uri, additional_headers)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mspace_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspace_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSPACE_KEY_ENVVAR_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mspace_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAuthError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidTypeAuthKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAuthError\u001b[0m: Arize Client could not obtain credentials. You can pass your api_key and space_key directly to the Arize Client, or you can set environment variables which will be read if the keys are not directly passed. To set the environment variables use the following variable names: \n - ARIZE_API_KEY for the api key\n - ARIZE_SPACE_KEY for the space key\nMissing: ['api_key', 'space_key']"
          ]
        }
      ],
      "source": [
        "## Arize API and Space Keys\n",
        "SPACE_KEY  = \"\"               # CTB / Kaggle: Wide LLMs\n",
        "API_KEY    = \"\"\n",
        "\n",
        "# INSTALL\n",
        "!pip install -q arize[AutoEmbeddings]\n",
        "\n",
        "from arize.pandas.logger import Client\n",
        "from arize.utils.types import Environments, ModelTypes, EmbeddingColumnNames, Schema\n",
        "\n",
        "# Arize Client\n",
        "arize_client = Client(space_key=SPACE_KEY, api_key=API_KEY)\n",
        "\n",
        "\n",
        "model_type = ModelTypes.REGRESSION\n",
        "if SPACE_KEY == \"SPACE_KEY\" or API_KEY == \"API_KEY\":\n",
        "    raise ValueError(\"❌ NEED TO CHANGE SPACE AND/OR API_KEY\")\n",
        "else:\n",
        "    print(\"✅ Import and Setup Arize Client Done! Now we can start using Arize!\")\n",
        "\n",
        "\n",
        "\n",
        "# Create generator for embedding vector\n",
        "# `generator` creates a vector from a prompt in the LLMs response surface.\n",
        "from arize.pandas.embeddings.tabular_generators import EmbeddingGeneratorForTabularFeatures\n",
        "import arize.pandas.embeddings.base_generators\n",
        "\n",
        "# EmbeddingGeneratorForTabularFeatures.list_pretrained_models()\n",
        "\n",
        "generator = EmbeddingGeneratorForTabularFeatures(\n",
        "    model_name=\"distilbert-base-uncased\",\n",
        "    tokenizer_max_length=512,\n",
        "    #, dropout=0                                                   # Remove Drop-out\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu01_pL0_Vbe"
      },
      "source": [
        "### Kaggle\n",
        "\n",
        "The easiest way to get the data is to connect directly is download it directly from Kaggle. This requires use of the `kaggle.json` file as described in the following articles.  \n",
        "\n",
        "References:\n",
        "* [Kaggle: House Prices - Advanced Regression Techniques](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/)\n",
        "* [Easiest way to download kaggle data in Google Colab](https://www.kaggle.com/general/74235)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUr9kiDKDr1H"
      },
      "outputs": [],
      "source": [
        "## Kaggle Set-up\n",
        "download_data = True\n",
        "kaggle_file = \"~/.kaggle/kaggle.json\"\n",
        "\n",
        "if not os.path.exists( '~/.kaggle' ):\n",
        "  ! pip install -q kaggle\n",
        "  from google.colab import files\n",
        "  ! mkdir ~/.kaggle\n",
        "\n",
        "if not os.path.isfile( kaggle_file ):\n",
        "    files.upload()   # produces a prompt!\n",
        "    ! cp kaggle.json ~/.kaggle/\n",
        "    ! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  # ! kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmC6xZoURTCB"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists( \"house-prices-advanced-regression-techniques/train.csv\" ):\n",
        "  ! kaggle competitions download -c house-prices-advanced-regression-techniques\n",
        "\n",
        "  ! mkdir house-prices-advanced-regression-techniques\n",
        "  ! unzip house-prices-advanced-regression-techniques.zip -d house-prices-advanced-regression-techniques\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmL3Sr4cJsGc"
      },
      "source": [
        "## II Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goXE4rl06Kr7"
      },
      "source": [
        "### Read Data\n",
        "\n",
        "Here we read the data from the locally stoded CSV files: `train.csv`, `test.csv` and `sample_submission.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4BgAX5cd7HH"
      },
      "outputs": [],
      "source": [
        "# Important Columns\n",
        "y_col = 'SalePrice'  # As it appears in the data\n",
        "id_col = 'Id'\n",
        "\n",
        "work_dir = 'house-prices-advanced-regression-techniques/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW5l-voSVw-c"
      },
      "outputs": [],
      "source": [
        "# Read Data\n",
        "train = pd.read_csv(\"house-prices-advanced-regression-techniques/train.csv\").drop('Id', axis=1)\n",
        "\n",
        "# test = Submissiion Data (e.g. Id, w/out SalePrice)\n",
        "test  = pd.read_csv(\"house-prices-advanced-regression-techniques/test.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anNwJRb0zHhU"
      },
      "source": [
        "### Column Names (Optional)\n",
        "\n",
        "LLM use prompts in\n",
        "Column Names are used to generate prompts. We can produce better prompts with cleaner column names, thus we get rid of the CamelCase.  This may be unnecessary as the data uses mostly industry standard codes likely to be correctly interpreted by various LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1nGeAKdWVQD"
      },
      "source": [
        "Because the LLM uses language, shorthand and abbreviations can be detriment from the model's ability to comprehend context. The data from the Kaggle competition contains column names in CamelCase. We make the minor adjustment to sidestep the programattic shorthand. We replace CamelCase column names with properly-spaced names prior to embedding.\n",
        "\n",
        "There are a number of other tweaks that can be made such as expanding \"Qual\" to \"Quality\" and \"Cond\" to \"Condition\".  For the time being we have not made these adjustments since the premise of the investigations is what could we get with relatively minor work.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jmyf7SuksM"
      },
      "source": [
        "> ME: In Real Estate what does \"ExterQual\" mean?\n",
        ">\n",
        "> ChatGPT: In real estate, \"ExterQual\" refers to the quality of the exterior materials and finishes of a home or property. It is often used by real estate agents, appraisers, and homebuyers to assess the overall condition and value of a property.\"ExterQual\" is typically included in a real estate listing or appraisal report, where it is used to describe the quality of the home's exterior materials and finishes. These may include siding, roofing, windows, doors, and other elements that make up the exterior of the home.\n",
        ">\n",
        "> The \"ExterQual\" rating is usually expressed as a letter grade, with higher grades indicating better quality materials and finishes. The grading system may vary depending on the specific market or region, but common grades include:\n",
        ">\n",
        "> Ex: Excellent\n",
        "> Gd: Good\n",
        "> TA: Typical/Average\n",
        "> Fa: Fair\n",
        "> Po: Poor\n",
        "> For example, a property with an \"ExterQual\" rating of \"Gd\" would be considered to have good quality exterior materials and finishes, while a property with a rating of \"Po\" would be considered to have poor quality materials and finishes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCqAWBULWTok"
      },
      "outputs": [],
      "source": [
        "def camel_case_split(s):\n",
        "  '''Converts Camel Case to Spaced String'''\n",
        "  idx = list(map(str.isupper, s))\n",
        "  # mark change of case\n",
        "  l = [0]\n",
        "  for (i, (x, y)) in enumerate(zip(idx, idx[1:])):\n",
        "      if x and not y:  # \"Ul\"\n",
        "          l.append(i)\n",
        "      elif not x and y:  # \"lU\"\n",
        "          l.append(i+1)\n",
        "  l.append(len(s))\n",
        "  # for \"lUl\", index of \"U\" will pop twice, have to filter that\n",
        "  return \" \".join([s[x:y] for x, y in zip(l, l[1:]) if x < y])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMujN9jIW1g3"
      },
      "outputs": [],
      "source": [
        "# Applies camel case split to each column turning, e.g. HousePrices -> \"House Prices\"\n",
        "for col in train.columns:\n",
        "  dct = {}\n",
        "  dct[col] = camel_case_split(col)\n",
        "  train = train.rename(columns=dct)\n",
        "\n",
        "for col in test.columns:\n",
        "  dct = {}\n",
        "  dct[col] = camel_case_split(col)\n",
        "  test  = test.rename(columns=dct)\n",
        "\n",
        "# Also our identifiers\n",
        "y_col = camel_case_split(y_col)\n",
        "id_col = camel_case_split(id_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A_TR9VF0RDA"
      },
      "source": [
        "### Model Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVpBlG8AePps"
      },
      "outputs": [],
      "source": [
        "# Define train_y, train_X, test_X\n",
        "\n",
        "# TRAIN\n",
        "train_y = train[ y_col ]                     # This will be split later\n",
        "train_X = train.drop(y_col , axis=1)\n",
        "\n",
        "# TEST\n",
        "test_X = test.drop(id_col, axis=1)           # Used for scoring and submission\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFyjbVUeVHD_"
      },
      "source": [
        "### Data Cleansing (None)\n",
        "\n",
        "No data cleansing is required including imputation of missing values. We use the data as is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5qifvSAU3IS"
      },
      "source": [
        "## III Feature Engineering (None)\n",
        "\n",
        "No traditional feature engineering is performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rYt8CJF3y5"
      },
      "source": [
        "### Split Rows for Embedding\n",
        "\n",
        "The token oken limit 512 token limit used by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWuMQZZ19AFv"
      },
      "outputs": [],
      "source": [
        "# Save names of columns\n",
        "tabular_columns = list(train_X.columns)  # List of train columns\n",
        "# X_cols = train.columns.drop( y_col )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEPHfTqJdzTn"
      },
      "source": [
        "## IV Create Prompts\n",
        "\n",
        "The novel idea of this demonstration is that the final embedding vectors from a large language model can be used as predictors in a traditional ML model.\n",
        "\n",
        "In this step, we use the arize Python package embedding methods to transform native columns to embeddings using Hugging Face's Distilbert LLM.\n",
        "\n",
        "The Arize package allows for a choice of pre-trained LLM implementations. We have found some variation across models, but have found Distilbert to provide a good trade-off of expense, speed, and general applicability.\n",
        "\n",
        "Arize's `generate_embeddding` function accepts DataFrame and creates a tabular embedding vector as a tuple of length 768. This embedding vector comes directly from the large language model as a tuple of length 768. The embedding vector can be thought of as a numerical location of the observation on the response hypersurface of the pre-trained LLM. It is this spatial location that is used as the basis of our anomaly detection.\n",
        "\n",
        "REFERENCE:\n",
        "[Distilbert Documentation](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
        "\n",
        "# ASIDE\n",
        "Optionally, the generate_embedding() function can produce an English text prompt for each row of your data for each data observation. For example, the prompt for one observation is:\n",
        "\n",
        "> The MS Sub Class is 60. The MS Zoning is RL. The Lot Frontage is 65.0. The Lot Area is 8450. The Street is Pave. The Alley is nan. The Lot Shape is Reg. The Land Contour is Lvl. The Utilities is AllPub. The Lot Config is Inside. The Land Slope is Gtl. The Neighborhood is CollgCr. The Condition1 is Norm. The Condition2 is Norm. The Bldg Type is 1Fam. The House Style is 2Story. The Overall Qual is 7. The Overall Cond is 5. The Year Built is 2003.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGO466F1hAqL"
      },
      "outputs": [],
      "source": [
        "# Concat  train_X and test_X together so that embeddings can be calculated at\n",
        "# the same time. This is possibly unnecessary and can be run separately with\n",
        "# the same prompts.\n",
        "\n",
        "# train_test = pd.concat([train_X, test_X], axis=0)  # (2919,80)\n",
        "\n",
        "# len_data_df = len(train_X)   # Remember how many are train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCsgLNw9UvKJ"
      },
      "outputs": [],
      "source": [
        "# Rows dont fit in *context window* so we split the rows into 4 groups\n",
        "split_prompt_n = 4  # Number of sets of columns\n",
        "\n",
        "the_cols    = list(train_X.columns)  # list of column names\n",
        "cols_per    = {}   # Dict key: int 0-n  value: list of cols in group\n",
        "prompt_ln   = len(the_cols)//split_prompt_n # Number of col per split\n",
        "\n",
        "for i in range(split_prompt_n):\n",
        "  if i != split_prompt_n - 1:\n",
        "    cols_per[str(i)] = the_cols[ prompt_ln*(i):prompt_ln*(i + 1) ]\n",
        "  else:\n",
        "    cols_per[str(i)] = the_cols[ prompt_ln*(i): ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBObnMqpZbx9"
      },
      "source": [
        "### Generate Embeddings\n",
        "\n",
        "Function `generate_embeddings` over each column set to create predictors for the models. These are then pasted onto the data frames for `train_X` and `test_X`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3LY-yWqPa-S"
      },
      "outputs": [],
      "source": [
        "# Avoid potential index errors\n",
        "# See note at: https://docs.arize.com/arize/embeddings/let-arize-generate-your-embeddings\n",
        "\n",
        "# train_test = train_test.reset_index(drop=True)\n",
        "train_X  = train_X.reset_index(drop=True)\n",
        "test_X   = test_X.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YE2aHZn3J8dg"
      },
      "outputs": [],
      "source": [
        "# Generate a set of embeddings for each split, the prompt window is 512 but\n",
        "# there are a decent number of columns so we split over 4 here\n",
        "\n",
        "tabular_vector_columns = []  # list of tabular vectors\n",
        "prompt_columns         = []  # list of prompt columns\n",
        "\n",
        "# Iterate over each column_set\n",
        "for i in range(split_prompt_n):\n",
        "  tab_vec_col_name_i = 'tabular_vector_' + str(i)\n",
        "  prompt_col_name_i = 'prompts_' + str(i)\n",
        "  tabular_vector_columns += [tab_vec_col_name_i]\n",
        "  prompt_columns += [prompt_col_name_i]\n",
        "\n",
        "  # train_X\n",
        "  train_X[tab_vec_col_name_i ], train_X[prompt_col_name_i] = generator.generate_embeddings(\n",
        "      train_X,\n",
        "      selected_columns  = cols_per[str(i)],\n",
        "      return_prompt_col = True\n",
        "  )\n",
        "\n",
        "  # test_X\n",
        "  test_X[tab_vec_col_name_i], test_X[prompt_col_name_i] = generator.generate_embeddings(\n",
        "    test_X,\n",
        "    selected_columns  = cols_per[str(i)],\n",
        "    return_prompt_col = True\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVTEq9QyOEIZ"
      },
      "source": [
        "### \"Explode\" Tabular Vectors into Columns\n",
        "\n",
        "Because the tabular vectors are Panda Series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSOkR3sjZCP1"
      },
      "outputs": [],
      "source": [
        "def explode( col, prefix ):\n",
        "  '''explodes single column embedding vector column to DataFrame'''\n",
        "  n_cols = len( col[0] )\n",
        "  col_names = [ prefix + str(i) for i in range(n_cols) ]\n",
        "\n",
        "  return( pd.DataFrame( col.to_list(), columns=col_names) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx_Fbvablpw0"
      },
      "outputs": [],
      "source": [
        "# Creates a column per embedding dimension for modeling\n",
        "# The template is vec_N_n, where N is the tabular vector, n is each dimension\n",
        "# of the tabular vector.\n",
        "\n",
        "# Adds columns: vec_N_n to DataFrames\n",
        "\n",
        "for i in range(split_prompt_n):\n",
        "  tab_vec_name = 'tabular_vector_' + str(i)    #\n",
        "  prefix = \"vec_\" + str(i) + \"_\"\n",
        "\n",
        "  # train_X\n",
        "  exploded = explode( train_X[ tab_vec_name], prefix )\n",
        "  train_X.loc[:, exploded.columns ] = exploded   # Idempotent replacement\n",
        "\n",
        "  # test_X\n",
        "  exploded = explode( test_X[ tab_vec_name], prefix )\n",
        "  test_X.loc[:, exploded.columns ] = exploded    # Idempotent replacement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT9wRihyNqWG"
      },
      "source": [
        "Embeddings are generated using the Arize Embeddings generator.\n",
        "Each row in the dataframe is a single prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cp7mfpCuVOv"
      },
      "source": [
        "## Train Test Split (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMR6KBzvT4lC"
      },
      "source": [
        "Use of `test_train_split` allows use to access some of the data to provide an evaluation data set.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TG4KQrYaVBpA"
      },
      "outputs": [],
      "source": [
        "# Warning: This is only done for internal evaluation;\n",
        "#   do not do this for final model submission. Train on full data\n",
        "\n",
        "train_X_sp, eval_X_sp, train_y_sp, eval_y_sp = train_test_split(train_X, train_y,\n",
        "                                               test_size=0.2,\n",
        "                                               random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2hrEGFadsHD"
      },
      "outputs": [],
      "source": [
        "# Drop everything that is not an exploded-out tabular-vector columns of the form\n",
        "# vec_N_n. Mig\n",
        "\n",
        "import re\n",
        "\n",
        "def get_matching_cols(df, regex):\n",
        "  r = re.compile(regex)\n",
        "  return( list( filter( r.match, df.columns) ) )\n",
        "\n",
        "def get_embedding_cols(df):\n",
        "  return( get_matching_cols(df, \"vec_\\d+_\\d+\") )\n",
        "\n",
        "\n",
        "embed_cols = get_embedding_cols( train_X )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJcUyd-GQ2MO"
      },
      "outputs": [],
      "source": [
        "# Which data is being used to fit?  train_X or train_X_sp?\n",
        "\n",
        "final = True;  # If final = `True` then no evaluation is availably\n",
        "\n",
        "X_cols = embed_cols\n",
        "\n",
        "if final:\n",
        "  fit_X = train_X[ X_cols ]\n",
        "  fit_y = train_y\n",
        "else:\n",
        "  fit_X = train_X_sp[ X_cols ]\n",
        "  fit_y = train_y_sp\n",
        "\n",
        "eval_X = eval_X_sp[ X_cols ]\n",
        "eval_y = eval_y_sp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0_pX8FkM76S"
      },
      "source": [
        "Arize Emeddings: The embeddings generator creates the Arize embeddings using LLMs. The LLMs use a preset prompt to generate embeddings for tabular datasets.\n",
        "\n",
        "Row uses the following prompt:\n",
        "The Lot Frontage is 80. The LotArea is 8034. The Street\tis Pave.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-gcFRdEPCNC"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDSBXW7kZ5HM"
      },
      "outputs": [],
      "source": [
        "# Define Evaluation metrics\n",
        "def evaluate(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true,y_pred)\n",
        "    mse = mean_squared_error(y_true,y_pred)\n",
        "    rsquare = r2_score(y_true,y_pred)\n",
        "    rmse = mean_squared_error(y_true,y_pred,squared = False)\n",
        "    try:\n",
        "        rmsle = mean_squared_log_error(y_true,y_pred,squared = False)\n",
        "    except:\n",
        "        rmsle = np.nan\n",
        "    return mae, mse, rsquare, rmse, rmsle\n",
        "\n",
        "def evaluate_show(y_true, y_pred):\n",
        "  '''display a table of the evaluation'''\n",
        "  score = evaluate( y_true, y_pred )\n",
        "  score_df = pd.DataFrame(score).T.round(5)\n",
        "\n",
        "  score_df.columns = ['MAE','MSE','R2 Square','RMSE','RMSLE']\n",
        "  score_df.style.set_properties(**{'background-color': 'aliceblue' ,'color':'black','border-color': '#8b8c8c'})\n",
        "  print(score_df)\n",
        "  return()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4D__u3BMTjJj"
      },
      "source": [
        "### Random Forest (Optional)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcAg0VGJe6Jm"
      },
      "source": [
        "#### Train RF  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq2Ch1RgpA2s"
      },
      "outputs": [],
      "source": [
        "## RF (Est. 5min)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model_rf = RandomForestRegressor(n_estimators=1000, max_depth=16, random_state=0)\n",
        "model_rf.fit(fit_X, fit_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDbvCVTae9B8"
      },
      "source": [
        "#### Evaluate RF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRASbEZBfL7N"
      },
      "outputs": [],
      "source": [
        "eval_y_pred = model_rf.predict(eval_X)\n",
        "\n",
        "score = evaluate(eval_y, eval_y_pred)\n",
        "score_df = pd.DataFrame(score).T.round(5)\n",
        "score_df.columns = ['MAE','MSE','R2 Square','RMSE','RMSLE']\n",
        "score_df.style.set_properties(**{'background-color': 'aliceblue' ,'color':'black','border-color': '#8b8c8c'})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrxWPC786Phx"
      },
      "source": [
        "#### Submit RF to Kaggle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZDTlhx56M5v"
      },
      "outputs": [],
      "source": [
        "# test_X = test_X[ embed_cols ] # .drop(tabular_vector_columns + prompt_columns + tabular_columns , axis=1)\n",
        "\n",
        "test_out = model_rf.predict( test_X )\n",
        "\n",
        "df_test_submission = pd.DataFrame( { 'Id': test.Id, 'SalePrice': test_out} )\n",
        "model_id = gen_model_id( model_type='rf')\n",
        "path = work_dir + model_id + '.csv'\n",
        "df_test_submission.to_csv( path, index=False )\n",
        "\n",
        "print( \"CSV file written to \" + path )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XozOTP-jkPlA"
      },
      "source": [
        "### XGBoost\n",
        "\n",
        "Create a straight-forward XGB regressor using squared error loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU0_t_snZ6PR"
      },
      "outputs": [],
      "source": [
        "# XGBoost (Est. 15m)\n",
        "import xgboost as xgb\n",
        "\n",
        "model_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\"\n",
        "                            , max_depth=6\n",
        "                            , n_estimators=10000\n",
        "                            , learning_rate=0.01\n",
        "                            , colsample_bytree=0.2\n",
        "                            , min_child_weight=1.5\n",
        "                            , reg_alpha=0.75\n",
        "                            , reg_lambda=0.45\n",
        "                            , subsample=0.6\n",
        "                             )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIWXINVtNkF0"
      },
      "source": [
        "#### Train XGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMSOOJj2aU8b"
      },
      "outputs": [],
      "source": [
        "# Train Model\n",
        "model = model_xgb.fit(fit_X,fit_y)   # est. 15m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HraYctinOGgV"
      },
      "source": [
        "#### Evaluate XGB\n",
        "\n",
        "The model appears to be overfitting. Internally, we measure a RMSLE of `0.155`. Kaggle has us at `0.194`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77nHO_5OOKRt"
      },
      "outputs": [],
      "source": [
        "eval_y_pred = model.predict(eval_X)\n",
        "# pred_by_model_train = model.predict(train_X_sp1)\n",
        "\n",
        "score = evaluate(eval_y, eval_y_pred)\n",
        "# score = [ math.trunc(score[i]*1000)/1000 for i in range(len(score)) ]\n",
        "score_df = pd.DataFrame(score).T.round(5)\n",
        "\n",
        "score_df.columns = ['MAE','MSE','R2 Square','RMSE','RMSLE']\n",
        "score_df.style.set_properties(**{'background-color': 'aliceblue' ,'color':'black','border-color': '#8b8c8c'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QGU4g26muyv"
      },
      "source": [
        "Best (2023-03-22)\n",
        "* MAE: 21678\n",
        "* R2: 0.76\n",
        "* RMLSE: 0.157"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IsA1yUEbQDx"
      },
      "source": [
        "#### Submit XGB to Kaggle\n",
        "\n",
        "Run the test set on the same data for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHD6smh5bPXt"
      },
      "outputs": [],
      "source": [
        "# Drop everything that is not a vector dimension column\n",
        "X_df_test1 = test_X[ X_cols ]\n",
        "\n",
        "test_out = model.predict(X_df_test1)\n",
        "df_test_submission = pd.DataFrame( { 'Id': test.Id, 'SalePrice': test_out} )\n",
        "model_id = gen_model_id( model_type='xgb-rmlse')\n",
        "\n",
        "path = work_dir + model_id + '.csv'\n",
        "df_test_submission.to_csv( path, index=False )\n",
        "\n",
        "print( \"CSV file written to \" + path )\n",
        "\n",
        "# kaggle competitions submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"Message\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF14nrNEyduT"
      },
      "source": [
        "## XGBOOST + orig data (-tk)\n",
        "\n",
        "In this model we allow training on the original data as ...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKQGnBk1Oj4z"
      },
      "source": [
        "## Sending Data To Arize for Analysis (Optional)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW5NkRlGhd1f"
      },
      "outputs": [],
      "source": [
        "#Add a prediction ID to data\n",
        "def add_prediction_id(df):\n",
        "    return [str(uuid.uuid4()) for _ in range(df.shape[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmG37xwfiLRx"
      },
      "outputs": [],
      "source": [
        "to_send_prod_df = data_test_df.copy().drop(vec_col_names, axis=1)\n",
        "to_train_prod_df = train_test.copy().drop(vec_col_names, axis=1)\n",
        "to_val_prod_df = eval_X_sp.copy().drop(vec_col_names, axis=1)\n",
        "\n",
        "to_send_prod_df['prediction_id'] = add_prediction_id(to_send_prod_df)\n",
        "to_train_prod_df['prediction_id'] = add_prediction_id(to_train_prod_df)\n",
        "to_val_prod_df['prediction_id'] = add_prediction_id(to_val_prod_df)\n",
        "\n",
        "to_send_prod_df['pred_score'] = 0\n",
        "to_train_prod_df['pred_score'] = 0\n",
        "to_val_prod_df['pred_score'] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3lOmWkckVDX"
      },
      "outputs": [],
      "source": [
        "model_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbh2yvr2hrz1"
      },
      "outputs": [],
      "source": [
        "features = tabular_columns\n",
        "\n",
        "embedding_features = {\n",
        "    # Dictionary keys will be name of embedding feature in the app\n",
        "    \"tabular embedding\": EmbeddingColumnNames(\n",
        "        vector_column_name=\"tabular_vector\",\n",
        "        data_column_name=\"prompt\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "# Define a Schema() object for Arize to pick up data from the correct columns for logging - think this fails silently for training\n",
        "test_schema = Schema(\n",
        "    prediction_id_column_name=\"prediction_id\",\n",
        "    prediction_score_column_name=\"pred_score\",\n",
        "    feature_column_names=features,\n",
        "    embedding_feature_column_names=embedding_features\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWHb9724huWd"
      },
      "outputs": [],
      "source": [
        "# Logging Production DataFrame\n",
        "response = arize_client.log(\n",
        "    dataframe=to_send_prod_df,\n",
        "    model_id=model_id,\n",
        "    model_version=\"v1\",\n",
        "    model_type=model_type,\n",
        "    environment=Environments.PRODUCTION,\n",
        "    schema=test_schema\n",
        ")\n",
        "\n",
        "if response.status_code != 200:\n",
        "    print(f\"❌ logging failed with response code {response.status_code}, {response.text}\")\n",
        "else:\n",
        "    print(f\"✅ You have successfully logged production set to Arize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmdzZm94iBGq"
      },
      "outputs": [],
      "source": [
        "# Logging Production DataFrame\n",
        "response = arize_client.log(\n",
        "    dataframe=to_train_prod_df,\n",
        "    model_id=model_id,\n",
        "    model_version=\"v1\",\n",
        "    model_type=model_type,\n",
        "    environment=Environments.TRAINING,\n",
        "    schema=test_schema\n",
        ")\n",
        "\n",
        "if response.status_code != 200:\n",
        "    print(f\"❌ logging failed with response code {response.status_code}, {response.text}\")\n",
        "else:\n",
        "    print(f\"✅ You have successfully logged production set to Arize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG2v2VonhnMf"
      },
      "outputs": [],
      "source": [
        "# Logging Production DataFrame\n",
        "response = arize_client.log(\n",
        "    dataframe=to_val_prod_df,\n",
        "    model_id=model_id,\n",
        "    model_version=\"v1\",\n",
        "    model_type=model_type,\n",
        "    batch_id=\"val1\",\n",
        "    environment=Environments.VALIDATION,\n",
        "    schema=test_schema\n",
        ")\n",
        "\n",
        "if response.status_code != 200:\n",
        "    print(f\"❌ logging failed with response code {response.status_code}, {response.text}\")\n",
        "else:\n",
        "    print(f\"✅ You have successfully logged production set to Arize\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu2sxs-KD6kV"
      },
      "source": [
        "## Results\n",
        "\n",
        "Achieves a Kaggle blind test score when submitted of RMLSE of 0.163"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnLCSXSwD95q"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "At first appearance, it would be surprising that a model without data, feature development, or training would produce results remotely this good.\n",
        "\n",
        "We think this is strongly indicative of what is to come from machine learning in the future. The history of machine learning has been guided by hackerish instincts without great underlying theory. Little is understood about how bagging, boosting, or various neural network architectures work. Practioners use these methods not because they are well understood, but because they work.\n",
        "\n",
        "In trying to improve models, data scientists are often tasked with finding datasets that are lacking.\n",
        "\n",
        "A big advantage of using large lanuage models is that the require little data cleaning. There is no need to impute missing values.  Missing value imputation is tricky.  Often this is done to satisfy the needs of the algorithm that do not handle missing values well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVS_C2YYekja"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGnB2lX-0SIG"
      },
      "outputs": [],
      "source": [
        "# # Zillow\n",
        "\n",
        "# # train_path = \"zillow/train_2017.csv\"\n",
        "# train_2017 = pd.read_csv( \"zillow/train_2017.csv\" )\n",
        "# train_2016 = pd.read_csv( \"zillow/train_2016_v2.csv\" )\n",
        "# train = pd.concat( [train_2016, train_2017] )\n",
        "\n",
        "# # properties_path = \"zillow/properties_2017.csv\"\n",
        "# prop_2017 = pd.read_csv(\"zillow/properties_2017.csv\")\n",
        "# prop_2016 = pd.read_csv(\"zillow/properties_2016.csv\")\n",
        "# props = pd.concat( [prop_2016, prop_2017] )\n",
        "\n",
        "\n",
        "# data.head().style.set_properties(**{'background-color': 'aliceblue' ,'color':'black','border-color': '#8b8c8c'})\n",
        "\n",
        "\n",
        "# props.sort_values(by=\"parcelid\")\n",
        "# props.head()\n",
        "\n",
        "# xdx = data.join( properties, on=\"parcelid\", how=\"left\", rsuffix=\"_r\" )\n",
        "# xdx.head()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuClass": "premium",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}